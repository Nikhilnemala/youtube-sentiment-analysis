{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x14S5ea1olxx"
      },
      "outputs": [],
      "source": [
        "#Problem Statement\n",
        "\"\"\"\n",
        "We have a huge number of comments from Youtube for a trailer from a worldwide\n",
        "production house, you as an AI serivice provider are supposed to analyse all the\n",
        "comments on that trailer, get the sentiment and the score, and give a consolidated\n",
        "report for the trailer about how it might perform on the box office.\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Libraries\n",
        "\"\"\"\n",
        "PyTorch - torch\n",
        "HuggingFace - transformers\n",
        "NLTK - nltk\n",
        "VADER - sentiment.vader\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "u31bD-1wqQCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "id": "LWujuc6ox5ea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "HajhXFQAyGLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "-bg_ttWBy3Dk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vaderSentiment"
      ],
      "metadata": {
        "id": "NMbvHS--zFmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_excel(\"/content/John Wick Comments.xlsx\")\n",
        "df\n"
      ],
      "metadata": {
        "id": "pFf-N4q6zdkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comments = []\n",
        "comments = df['Comments'].tolist()"
      ],
      "metadata": {
        "id": "wrhGWwm37hnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for comment in comments:\n",
        "  print(comment)\n",
        "  print(\"===\")"
      ],
      "metadata": {
        "id": "cpiawy5d8Bks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\"\"\"\n",
        "What are stopwords?\n",
        "Words that help make up a sentence but do not have their own meaning\n",
        "it, they, them, what, am, I\n",
        "\"\"\"\n",
        "comment_score = sia.polarity_scores(\"I am very happy with this movie.\")\n",
        "print(comment_score)"
      ],
      "metadata": {
        "id": "YIxFHmgb8NK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for classifying my comments into positive and negative, i used an AI model from HuggingFace to get the score of the sentence, we will go with nltk/vader sentiment"
      ],
      "metadata": {
        "id": "w9H5oHSdmXRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "7_sJAOcHoJKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Today is a very sunny day, I would like to go out and play football with my friends\"\n",
        "tokenized_comment = word_tokenize(sentence)\n",
        "print(\"Tokenized Comment: \",tokenized_comment)\n",
        "\n",
        "processed_comment = [ word for word in tokenized_comment if word.lower() not in stop_words]\n",
        "print(\"Processed Comment: \",processed_comment)"
      ],
      "metadata": {
        "id": "8SfrreElop5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(raw_comment):\n",
        "  tokenized_comment = word_tokenize(sentence)\n",
        "  processed_comment = [ word for word in tokenized_comment if word.lower() not in stop_words]\n",
        "  return ' '.join(processed_comment)"
      ],
      "metadata": {
        "id": "Tjch50BXp21e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = remove_stopwords(\"Today is a very sunny day, I would like to go out and play football with my friends\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "oJ78lSNirFMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "aU5lGdVirWI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "LaBSFTJbuyjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "classifier = pipeline(\"sentiment-analysis\", model = \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")"
      ],
      "metadata": {
        "id": "f0fjZb-Hu1nn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_excel(\"/content/John Wick Comments.xlsx\")"
      ],
      "metadata": {
        "id": "wjEW-5fGvrsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comments = []\n",
        "comments = df['Comments'].tolist()"
      ],
      "metadata": {
        "id": "vMfd654iv0Ae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(raw_comment):\n",
        "  tokenized_comment = word_tokenize(raw_comment)\n",
        "  processed_comment = [ word for word in tokenized_comment if word.lower() not in stop_words]\n",
        "  return ' '.join(processed_comment)"
      ],
      "metadata": {
        "id": "GeYfC94wwB_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_comment_sentiment_details(raw_comment):\n",
        "  processed_comment = remove_stopwords(raw_comment)\n",
        "\n",
        "  words = processed_comment.split()\n",
        "  positive_words = \"\"\n",
        "  negative_words = \"\"\n",
        "  comment_sentiment = \"\"\n",
        "\n",
        "  sentence_score_temp = sia.polarity_scores(processed_comment)\n",
        "  abs_sentence_score = abs(sentence_score_temp['compound'])\n",
        "  sentiment_label = classifier(processed_comment)\n",
        "  comment_sentiment = sentiment_label[0]['label']\n",
        "\n",
        "  if abs_sentence_score == 0:\n",
        "    comment_sentiment = \"NEUTRAL\"\n",
        "\n",
        "  if comment_sentiment == \"NEGATIVE\":\n",
        "    sentence_score = abs_sentence_score * -1\n",
        "    for word in  words:\n",
        "      word_sentiment = sia.polarity_scores(word)\n",
        "      if word_sentiment['compound'] < 0:\n",
        "        negative_words += word + \" \"\n",
        "\n",
        "  if comment_sentiment == \"NEGATIVE\":\n",
        "    sentence_score = abs_sentence_score\n",
        "    for word in  words:\n",
        "      word_sentiment = sia.polarity_scores(word)\n",
        "      if word_sentiment['compound'] > 0:\n",
        "        positive_words += word + \" \"\n",
        "  else:\n",
        "    sentence_score = abs_sentence_score\n",
        "\n",
        "  return positive_words, negative_words, sentence_score, comment_sentiment"
      ],
      "metadata": {
        "id": "nKqyJNWzwQnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_words = \"\"\n",
        "negative_words = \"\"\n",
        "neu_count = 0\n",
        "\n",
        "pos_values_list = []\n",
        "neg_values_list = []\n",
        "avg_pos_score = 0\n",
        "avg_neg_score = 0\n",
        "\n",
        "for comment in comments:\n",
        "  pw, nw, ss, cs = get_comment_sentiment_details(comment)\n",
        "  positive_words += pw+ \" \"\n",
        "  negative_words += nw+ \" \"\n",
        "\n",
        "  if cs == \"NEGATIVE\":\n",
        "    neg_values_list.append(ss)\n",
        "  elif cs == \"POSITIVE\":\n",
        "    pos_values_list.append(ss)\n",
        "  else:\n",
        "    neu_count+=1\n",
        "\n",
        "try:\n",
        "  avg_pos_score = sum(pos_values_list) / len(pos_values_list)\n",
        "  avg_neg_score = sum(neg_values_list) / len(neg_values_list)\n",
        "except ZeroDivisionError:\n",
        "  if len(pos_values_list) == 0 or len(neg_values_list) == 0:\n",
        "    avg_pos_score = 0\n",
        "    avg_neg_score = 0\n",
        "\n",
        "final_score = (avg_pos_score + avg_neg_score) / (len(pos_values_list) + len(neg_values_list))\n",
        "print(final_score)"
      ],
      "metadata": {
        "id": "Cyrh6Fp0zXO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_words"
      ],
      "metadata": {
        "id": "tAA9p2wB47JD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "negative_words"
      ],
      "metadata": {
        "id": "v0JUjTAl7PBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_pos_score"
      ],
      "metadata": {
        "id": "stsbewDa7QpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_pos_score"
      ],
      "metadata": {
        "id": "aeM4jO_17SsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wordcloud"
      ],
      "metadata": {
        "id": "r4ZUnCjw7UTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"positives\")\n",
        "wordcloud_positive = WordCloud(width=800, height=400, background_color='white').generate(positive_words)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.imshow(wordcloud_positive, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pbHLEzK07Ypj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"negatives\")\n",
        "wordcloud_negative = WordCloud(width=800, height=400, background_color='white').generate(negative_words)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.imshow(wordcloud_negative, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KnZnwFja7oIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_score = (avg_pos_score + avg_neg_score) / (len(pos_values_list) + len(neg_values_list))\n",
        "print(final_score)"
      ],
      "metadata": {
        "id": "9rZrPdr47vvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if final_score >= 0.5:\n",
        "    verdict = \"Hit\"\n",
        "elif final_score >= 0.2:\n",
        "    verdict = \"Average\"\n",
        "else:\n",
        "    verdict = \"Flop\"\n",
        "\n",
        "print(f\"Verdict: {verdict}\")"
      ],
      "metadata": {
        "id": "r5qIJ2p1yyk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df['comment_sentiment'] = df['Comments'].apply(lambda comment: get_comment_sentiment_details(comment)[3])\n",
        "\n",
        "sns.countplot(x='comment_sentiment', data=df)\n",
        "plt.title('Sentiment Distribution of John Wick Trailer Comments')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4vmYQ6YyzN7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}